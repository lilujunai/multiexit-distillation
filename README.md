# Distillation-Based Training for Multi-Exit Architectures (<a target="_blank" href="https://mary-phuong.github.io">Mary Phuong</a>, <a target="_blank" href="http://pub.ist.ac.at/~chl/">Christoph H. Lampert</a>, ICCV 2019)

We present a new method for training *multi-exit architectures*.
A multi-exit architecture looks like this:

![Multi-exit architecture](https://github.com/mary-phuong/multiexit-distillation/images/multiexit_architecture.png)

We propose to train such architectures by transferring knowledge from late exits to early exits, via so-called *distillation*.

Read more [here](https://mary-phuong.github.io/multiexit_distillation.pdf).
